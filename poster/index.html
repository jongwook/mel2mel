<!doctype html>
<html>
<head>
<title>Neural Music Synthesis</title>
<meta name="viewport" content="width=550, initial-scale=1">
<link rel='stylesheet' type='text/css' href='poster.css'>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js" integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js" integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>
<body>
<div id="container">
    <h1>Neural Music Synthesis</h1>
    <h2 id="name">Jong Wook Kim @ MiQ</h2>
    <h2 id="email">jongwook@nyu.edu</h2>
    <hr>
    <h3>About the MiQ Lab</h3>
    Understanding music at scale to enhance the Spotify platform. Visit <a href="http://miq.spotify.net">miq.spotify.net</a>!
    <ul>
        <li><b>Content Description</b>: rhythm, melody, structure, instrument, language, etc.</li>
        <li><b>Audio Processing</b>: auto-mixing, karaoke, style transfer, etc.</li>
        <li><b>Audio Marketplace</b>: skip minimization, originality estimation, etc.</li>
    </ul>
    <h3>Motivation and Background</h3>
    <ul>
        <li>We want to be able to convert audio into disentangled semantic information and vice versa:</li>
    </ul>
    <center><img src="images/ae.jpg" width="420px"></center>
    <ul>
        <li>End-to-end neural speech synthesis using WaveNet<cite>1</cite> produces near-perfect sound</li>
        <li>WaveNet is successfully applied in musical note synthesis<cite>3</cite> and music translation<cite>6</cite></li>
    </ul>

    <h3>Autogressive Music Synthesis using WaveNet</h3>
    <center><img src="images/wavenet_conv_gif.gif" align="left" width="220px"></center>
    <ul style="margin-left: 10px">
        <li>
            Built real-time audio synthesis using
            <a href="https://devblogs.nvidia.com/nv-wavenet-gpu-speech-synthesis/">nv-wavenet</a>,
            NVIDIA's official WaveNet implementation, following Deep Voice<cite>2</cite>.
        </li>
        <li>
            Implemented a CGI server that can run WaveNet inference and stream the audio to the browser in real-time. 
        </li>
    </ul>
    <h3>Dataset</h3>
    <ul>
        <li>
            334 MIDI files of classical piano music crawled from <a href="http://www.piano-midi.de/">www.piano-midi.de</a>
        </li>
        <li>
            Synthesized clean audio for training, using the default soundfont of <a href="https://musescore.org">MuseScore</a>
        </li>
        <!--<li>
            GM Instruments:
            Grand Piano,
            Electric Piano,
            Vibraphone,
            Church Organ,
            Acoustic Guitar,
            Pizzicato Strings,
            Orchestral Harp,
            String Ensemble,
            Trumpet,
            Synth Lead
        </li>-->
    </ul>
    <h2>Methodology</h2>
    <h3>Overall Architecture</h3>
    <center><img src="images/architecture.svg" width="500px"></center>
    <ul>
        <li>MIDI data and instrument information are combined to predict 80-dim Mel spectrogram</li>
        <li>WaveNet synthesizes music, by conditioning on the predicted Mel spectrogram </li>
    </ul>
    <h3>The Onsets and Frames Salience Representation</h3>
    <ul>
        <li>
            Inspired by the onsets and frames paper<cite>5</cite>, the input has 2 \(\times\) 88 channels,
            where the first 88 indicate the onset timings and the last 88 indicate the frames.
        </li>
        <li>
            The input matrix is sparse, and the nonzero elements contain the MIDI velocity.
        </li>
    </ul>
    <img src="images/FiLM.png" align="right" width="120px">
    <h3>FiLM Layer for Timbre Conditioning</h3>
    <ul>
        <li>
            FiLM layers<cite>4</cite> allow the network to take <b>side information</b>
        </li>
        <li>
            FiLM layers learn the functions \(f\) and \(h\) which take the
            conditioning input (in our case, the timbre embedding) and 
            produce the affine transformation parameters:
        </li>
    </ul>
    \[
        \gamma_{i, c} = f_c ( \mathbf{x}_i ), ~~~~ \beta_{i, c} = h_c ( \mathbf{x}_i ).
    \]
    <ul>
        <li>
            The features \(\mathbf{F}_{i, c}\) are then transformed element-wise:
        </li>
    </ul>
    \[
        \mathrm{FiLM}(\mathbf{F}_{i,c} | \gamma_{i, c}, \beta_{i, c}) 
        = \gamma_{i,c} \mathbf{F}_{i, c} + \beta_{i, c}.
    \]
    <h2>Results</h2>
    <h3>Generated Mel Spectrograms</h3>
    <center><video src="images/video.mp4" width="480px" loop controls></video></center>
    <h3>Reconstruction Accuracy w.r.t. Loss Functions and Instruments</h3>
    <center>
        <img src="images/correlations.svg" width="240px">
        <img src="images/correlations-instrument.svg" width="240px">
    </center>
    <ul>
        <li>significant degradation at each stage (Mel prediction, WaveNet, Âµ-law)</li>
        <li>Reconstruction is generally better at higher frequencies</li>
        <li>Accuracy depends on both timbre and frequency</li>
    </ul>
    <h3>Embedding Space Visualization</h3>
    <center>
        <img src="images/centroid.svg" width="240px">
        <img src="images/energy.svg" width="240px">
    </center>
    <h3>Mean Opinion Scores of Synthesis</h3>
    <center>
        <table border cellspaing="0" cellpadding="0">
            <thead>
                <tr>
                    <th>Condition</th> 
                    <th>Scores \(\pm\) SEM</th>
                    <th>Condition</th> 
                    <th>Scores \(\pm\) SEM</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Original</td>
                    <td>\(4.316 \pm 0.041 \)</td>
                    <td>tanh-log-abs MSE</td>
                    <td>\(3.183 \pm 0.056 \)</td>
                </tr>
                <tr>
                    <td>mu-law Only</td>
                    <td>\(3.879 \pm 0.051 \)</td>
                    <td>log-abs MSE</td>
                    <td>\(3.011 \pm 0.058 \)</td>
                </tr>
                <tr>
                    <td>Ground-Truth Mel</td>
                    <td>\(3.378 \pm 0.052 \)</td>
                    <td>abs MSE</td>
                    <td>\( 2.757 \pm 0.058 \)</td>
                </tr>
            </tbody>
        </table>
    </center>
    <!--
    <pre>
        GP 40 3.438 0.164
        EP 41 2.878 0.185
        VP 40 2.950 0.165
        CO 41 3.402 0.177
        AG 41 3.305 0.161
        PS 41 3.268 0.181
        OH 42 3.060 0.193
        SE 41 2.915 0.166
        TP 36 3.250 0.195
        SL 41 3.378 0.174            
    </pre>
    -->
    <hr>
    <h2>Conclusions</h2>
    <ul>
        <li>Built a model which combines RNN and WaveNet to synthesize polyphonic music from MIDI score and instrument information.</li>
        <li>Can interpolate between instruments in the embedding space, making interesting sounds</li>
        <li>The embedding space captures the distribution of timbre, such as spectral centroid and mean energy</li>
    </ul>
    <hr>
    <h2>Future Work</h2>
    <b>Upcoming Blog &amp; Paper</b>
    <ul>
        <li>Ablation study for architecutre variations: number of FiLM and Conv layers, activations, etc.</li>
        <li>Embedding space visualization for 100+ instruments</li>
        <li>ClariNet<cite>7</cite>: continuous and paralllelizable WaveNet based on Gaussian IAF</li>
    </ul>
    <b>Further Research Directions</b>
    <ul>
        <li>Learning the inverse of the network, for music transcription </li>
        <li>Using GAN to generate higher-dimensional representations for better synthesis</li>
    </ul>
    <hr>
    <h2 id="references-header">References</h2>
    <ul id="references">
        <li>Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.</li>
        <li>Arik, S. O., Chrzanowski, M., Coates, A., Diamos, G., Gibiansky, A., Kang, Y., ... & Sengupta, S. (2017). Deep voice: Real-time neural text-to-speech. arXiv preprint arXiv:1702.07825.</li>
        <li>Engel, J., Resnick, C., Roberts, A., Dieleman, S., Eck, D., Simonyan, K., & Norouzi, M. (2017). Neural audio synthesis of musical notes with wavenet autoencoders. arXiv preprint arXiv:1704.01279.</li>    
        <li>Perez, E., Strub, F., De Vries, H., Dumoulin, V., & Courville, A. (2017). Film: Visual reasoning with a general conditioning layer. arXiv preprint arXiv:1709.07871.</li>
        <li>Hawthorne, C., Elsen, E., Song, J., Roberts, A., Simon, I., Raffel, C., ... & Eck, D. (2017). Onsets and frames: Dual-objective piano transcription. arXiv preprint arXiv:1710.11153.</li>
        <li>Mor, N., Wolf, L., Polyak, A., & Taigman, Y. (2018). A Universal Music Translation Network. arXiv preprint arXiv:1805.07848.</li>
        <li>Ping, W., Peng, K., & Chen, J. (2018). ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech. arXiv preprint arXiv:1807.07281.</li>
    </ul>
</div>
</body>
</html>
